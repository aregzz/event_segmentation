{
 "metadata": {
  "name": "",
  "signature": "sha256:b7b1242eea8283aecf568c44e1da956db5118a7703bc5573c5ffac399a70143a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayesian Online Changepoint Detection\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Table of Contents\n",
      "\n",
      "1. [Introduction](#introdution)\n",
      "2. [The BOCPD Algorithm](#the_bocpd_algorithm)\n",
      "3. [Python Implementation](#python_implementation)\n",
      "4. [Conclusion](#conclusion)\n",
      "5. [References](#references)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"introduction\"></a>\n",
      "## 1. Introduction\n",
      "\n",
      "Change points can be defined as abrupt changes in the generative parameters of a sequence of data. This type of behavior is characteristic of non-stationary time series, and it is essentially why modelling such processes can be, in general, so challenging. When modelling non-stationary time series, taking into account all data points observed so far to, for instance, make a prediction about the next data point, do not necessary mean that the predictive performance will be higher than the one that we could obtain by taking into account only a recent subset of all observations. Change Point Detection (CPD) methods address this problem by firstly identifying regime change events, i.e., change points, and by secondly adapting the predictive model appropriately. The Bayesian Online CPD (BOCPD) algorithm, introduced by Adams and MacKay [1] in 2007, is one of those methods and it will be the main subject of this article.\n",
      "\n",
      "This article is organized as follows. In the section 2, I review some of the details and derivations of the BOCPD algorithm.\n",
      "A simple Python implementation of this algorithm and experimental results on synthetic data are presented in section 3. Finally, I present the conclusions of this work in section 4. The references used throughout this work can be found under section 5."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"the_bocpd_algorithm\"></a>\n",
      "## 2. The BOCPD Algorithm\n",
      "\n",
      "As one can expect, the BOCPD algorithm is a Bayesian approach to the change point detection problem. Central to this method is the notion of run length. The run length is defined as a random variable that represents the time since the last change point occurred. As we will soon see, given an underlying predictive model (UPM) and a hazard function, one can perform exact inference about the run length at each time step in a sequential way. Assuming for now that we can compute the posterior probability of the run length, i.e. $p(r_t|x_{1:t})$, where $r_t$ refer to the run length at time $t$ and $x_{1:t}$ are all the data points observed up to time $t$, the predictive probability distribution of the next data point is obtained, according to the Bayesian framework, by integrating out the uncertainty about $r_t$, i.e.,\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "p(x_{t+1}|x_{1:t}) & = \\sum_{r_t = 0}^{t} p(x_{t+1}, r_t|x_{1:t}) \\\\\n",
      "& = \\sum_{r_t = 0}^{t} p(x_{t+1}| r_t, x_{1:t}) p(r_t| x_{1:t}).\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "The probability $p(x_{t+1}| r_t, x_{1:t})$ is computed using the UPM, therefore, we only need to compute $p(r_t| x_{1:t})$ to apply the above equation. This is exactly when the BOCPD algorithm comes in handy. BOCPD allows us to compute $p(r_t, x_{1:t})$ recursively as follows,\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "p(r_t, x_{1:t}) & = \\sum_{r_{t-1} = 0}^{t-1} p(r_t, r_{t-1}, x_{1:t}) \\\\\n",
      "& = \\sum_{r_{t-1} = 0}^{t-1} p(r_t, x_t, r_{t-1}, x_{1:t-1}) \\\\\n",
      "& = \\sum_{r_{t-1} = 0}^{t-1} p(r_t, x_t| r_{t-1}, x_{1:t-1}) p(r_{t-1}, x_{1:t-1})\\\\\n",
      "& = \\sum_{r_{t-1} = 0}^{t-1} p(r_t| r_{t-1}, x_{1:t}) p(x_t| r_{t-1}, x_{1:t-1}) p(r_{t-1}, x_{1:t-1}).\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "Note that $p(r_t| r_{t-1}, x_{1:t})$ is the conditional prior on the run length $r_t$, therefore, it is independent from the observations and consequently $p(r_t| r_{t-1}, x_{1:t}) = p(r_t| r_{t-1})$. Given $r_{t-1}$, the probability $p(x_t| r_{t-1}, x_{1:t-1})$ also does not depend on all data points observed so far, but only on the recent subset defined by $r_{t-1}$. This leads to $p(x_t| r_{t-1}, x_{1:t-1}) = p(x_t| r_{t-1}, x_t^{(r)})$, where $x_t^{(r)} = x_{t-r:t-1}$. Applying these results to the above equation, we get the final result\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "\\begin{split}\n",
      "p(r_t, x_{1:t}) & = \\sum_{r_{t-1} = 0}^{t-1} p(r_t| r_{t-1}) p(x_t| r_{t-1}, x_t^{(r)}) p(r_{t-1}, x_{1:t-1}).\n",
      "\\end{split}\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "The conditional prior on $r_t$ is defined by\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "p(r_t| r_{t-1}) =\n",
      "\\begin{cases}\n",
      "H(r_{t-1}+1) & \\mbox{if } r_t = 0 \\\\\n",
      "1 - H(r_{t-1}+1) & \\mbox{if } r_t = r_{t-1} + 1 \\\\\n",
      "0 & \\mbox{otherwise}\n",
      "\\end{cases}\n",
      ",\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "where $H(\\tau)$ is a hazard function. As we can see, given $r_{t-1}$, the random variable $r_t$ has only two possible outcomes, either $r_t$ increase and $r_t = r_{t-1}+1$ or a change point occur and $r_t = 0$, and therefore, the probability distribution assigns a zero probability to all other combinations. Essentially, this is the property that gives the BOCPD algorithm its computational efficiency.\n",
      "\n",
      "By definition a hazard function is given by\n",
      "\n",
      "$$\n",
      "\\begin{equation}\n",
      "H(\\tau) = \\frac{p_{event}(g=\\tau)} {\\sum_{t=\\tau}^{\\infty} p_{event}(g=t)}\n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "where $p_{event}(g=\\tau)$ is the probability of the occurrence of some event at time $\\tau$. In this context, the hazard function $H(\\tau)$ can be interpreted as the instantaneous rate of occurrence of changepoints given that no changepoint occurred up to time $\\tau$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"python_implementation\"></a>\n",
      "## 3. Python Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"conclusion\"></a>\n",
      "## 4. Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"references\"></a> \n",
      "## 5. References\n",
      "\n",
      "\n",
      "\n",
      "[1] Adams, R. P. and MacKay, D. J. C. Bayesian online changepoint detection. Technial report, University of Cambridge, Cambridge, UK, 2007.\n",
      "\n",
      "[2] Saat\u00e7i, Y., Turner, R., and Rasmussen, C. E. (2010). Gaussian process change point models. In _27th_ _International_ _Conference_ _on_ _Machine_ _Learning_, pages 927-934, Haifa, Israel. Omnipress.\n",
      "\n",
      "[3] Kevin P. Murphy. Conjugate Bayesian analysis of the Gaussian distribution. Privately Published, 2007."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}